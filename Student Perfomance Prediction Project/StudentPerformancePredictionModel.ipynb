{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9882938e-c20d-4ab4-bbd8-c516aa5e8b64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   student_id  age  gender previous_education  study_hours  sleep_hours  \\\n",
      "0           1   24  Female        High School     4.394050     9.510559   \n",
      "1           2   21  Female        High School     6.129396     9.705572   \n",
      "2           3   28  Female           Bachelor     9.430758     7.462736   \n",
      "3           4   25    Male          Associate     2.406927     6.142727   \n",
      "4           5   22  Female        High School     1.215014     8.725295   \n",
      "\n",
      "   attendance_rate family_income parent_education extracurricular_activities  \\\n",
      "0         0.737798        Medium          Primary                         No   \n",
      "1         0.703874           Low           Higher                         No   \n",
      "2         0.580609          High           Higher                        Yes   \n",
      "3         0.828205           Low        Secondary                         No   \n",
      "4         0.985575          High        Secondary                         No   \n",
      "\n",
      "  study_group  stress_level  online_courses internet_access  travel_time  \\\n",
      "0         Yes             9               1             Yes     1.052750   \n",
      "1         Yes             5               3             Yes     1.089575   \n",
      "2         Yes             4               2             Yes     0.951897   \n",
      "3          No             2               1              No     0.586568   \n",
      "4          No             6               3              No     0.328647   \n",
      "\n",
      "   performance performance_category  \n",
      "0    98.183431                    A  \n",
      "1    84.072116                    B  \n",
      "2    75.233024                    C  \n",
      "3    47.389229                    F  \n",
      "4    67.574216                    D  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 17 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   student_id                  1000 non-null   int64  \n",
      " 1   age                         1000 non-null   int64  \n",
      " 2   gender                      1000 non-null   object \n",
      " 3   previous_education          1000 non-null   object \n",
      " 4   study_hours                 1000 non-null   float64\n",
      " 5   sleep_hours                 1000 non-null   float64\n",
      " 6   attendance_rate             1000 non-null   float64\n",
      " 7   family_income               1000 non-null   object \n",
      " 8   parent_education            1000 non-null   object \n",
      " 9   extracurricular_activities  1000 non-null   object \n",
      " 10  study_group                 1000 non-null   object \n",
      " 11  stress_level                1000 non-null   int64  \n",
      " 12  online_courses              1000 non-null   int64  \n",
      " 13  internet_access             1000 non-null   object \n",
      " 14  travel_time                 1000 non-null   float64\n",
      " 15  performance                 1000 non-null   float64\n",
      " 16  performance_category        998 non-null    object \n",
      "dtypes: float64(5), int64(4), object(8)\n",
      "memory usage: 132.9+ KB\n",
      "None\n",
      "Classification Model:\n",
      "Accuracy: 0.64\n",
      "\n",
      "Regression Model:\n",
      "Mean Squared Error: 112.60\n",
      "R2 Score: 0.81\n",
      "\n",
      "Classification Model Feature Importance Analysis:\n",
      "Top 10 most important features:\n",
      "gender: 0.1165\n",
      "family_income: 0.1136\n",
      "age: 0.1056\n",
      "previous_education: 0.1035\n",
      "study_hours: 0.1025\n",
      "sleep_hours: 0.0830\n",
      "attendance_rate: 0.0757\n",
      "student_id: 0.0703\n",
      "travel_time: 0.0140\n",
      "parent_education: 0.0137\n",
      "\n",
      "Regression Model Feature Importance Analysis:\n",
      "Top 10 most important features:\n",
      "age: 0.1912\n",
      "gender: 0.1544\n",
      "family_income: 0.1341\n",
      "study_hours: 0.1273\n",
      "previous_education: 0.1202\n",
      "attendance_rate: 0.1188\n",
      "sleep_hours: 0.0939\n",
      "student_id: 0.0200\n",
      "stress_level: 0.0030\n",
      "study_group: 0.0029\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "\n",
    "# 1. Data Loading and Exploration\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(data.head())\n",
    "    print(data.info())\n",
    "    return data\n",
    "\n",
    "# 2. Data Preprocessing\n",
    "def preprocess_data(data):\n",
    "    # Identify numeric and categorical columns\n",
    "    numeric_features = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Create preprocessing pipelines\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # Combine preprocessing steps\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "# 3. Model Training and Evaluation\n",
    "def train_and_evaluate_model(X, y, preprocessor, is_classification=True):\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create a pipeline with preprocessor and model\n",
    "    if is_classification:\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    else:\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('model', model)])\n",
    "\n",
    "    # Fit the pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    if is_classification:\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    else:\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "        print(f\"R2 Score: {r2:.2f}\")\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "# 4. Feature Importance Analysis\n",
    "def analyze_feature_importance(pipeline, feature_names):\n",
    "    model = pipeline.named_steps['model']\n",
    "    feature_importance = model.feature_importances_\n",
    "    feature_importance_dict = dict(zip(feature_names, feature_importance))\n",
    "    sorted_features = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"Top 10 most important features:\")\n",
    "    for feature, importance in sorted_features[:10]:\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the data\n",
    "    data = load_data(r\"C:\\Users\\ADMIN\\Data Analytics Projects\\Student Perfomance Prediction Project\\student_data.csv\")\n",
    "\n",
    "    \n",
    "\n",
    "    ###################################################################################\n",
    "\n",
    "    # Add this after loading the data\n",
    "    data['performance_category'] = data['performance_category'].fillna(data['performance_category'].mode()[0])\n",
    "\n",
    "\n",
    "    # In the main execution block, change these lines:\n",
    "    X = data.drop([\"performance\", \"performance_category\"], axis=1)\n",
    "    y_regression = data[\"performance\"]\n",
    "    y_classification = data[\"performance_category\"]\n",
    "\n",
    "    # Create preprocessor here\n",
    "    preprocessor = preprocess_data(X)\n",
    "    \n",
    "    # Train and evaluate classification model\n",
    "    print(\"Classification Model:\")\n",
    "    classification_pipeline = train_and_evaluate_model(X, y_classification, preprocessor, is_classification=True)\n",
    "    \n",
    "    # Train and evaluate regression model\n",
    "    print(\"\\nRegression Model:\")\n",
    "    regression_pipeline = train_and_evaluate_model(X, y_regression, preprocessor, is_classification=False)\n",
    "    \n",
    "    # Analyze feature importance for both models\n",
    "    print(\"\\nClassification Model Feature Importance Analysis:\")\n",
    "    analyze_feature_importance(classification_pipeline, X.columns)\n",
    "    \n",
    "    print(\"\\nRegression Model Feature Importance Analysis:\")\n",
    "    analyze_feature_importance(regression_pipeline, X.columns)\n",
    "    ###################################################################################\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aedbef1-a6c6-4208-b927-f0b73d1541f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ... (keep your existing import statements and functions)\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "\n",
    "def train_and_evaluate_model(X, y, preprocessor, model, param_grid=None):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Apply SMOTE to handle class imbalance\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('model', model)])\n",
    "    \n",
    "    if param_grid:\n",
    "        grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "        best_model = grid_search.best_estimator_\n",
    "    else:\n",
    "        best_model = pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ... (keep your existing data loading code)\n",
    "    data = load_data(r\"C:\\Users\\ADMIN\\Data Analytics Projects\\Student Perfomance Prediction Project\\student_data.csv\")\n",
    "\n",
    "    # Add this after loading the data\n",
    "    data['performance_category'] = data['performance_category'].fillna(data['performance_category'].mode()[0])\n",
    "    \n",
    "    # Feature engineering\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    feature_names = poly.get_feature_names(X.columns)\n",
    "    X_engineered = pd.DataFrame(X_poly, columns=feature_names)\n",
    "    \n",
    "    preprocessor = preprocess_data(X_engineered)\n",
    "    \n",
    "    # Try different algorithms with hyperparameter tuning\n",
    "    models = {\n",
    "        'GradientBoosting': (GradientBoostingClassifier(), {\n",
    "            'model__n_estimators': [100, 200],\n",
    "            'model__learning_rate': [0.01, 0.1],\n",
    "            'model__max_depth': [3, 5]\n",
    "        }),\n",
    "        'SVM': (SVC(), {\n",
    "            'model__C': [0.1, 1, 10],\n",
    "            'model__kernel': ['rbf', 'poly']\n",
    "        }),\n",
    "        'NeuralNetwork': (MLPClassifier(), {\n",
    "            'model__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "            'model__alpha': [0.0001, 0.001, 0.01]\n",
    "        })\n",
    "    }\n",
    "    \n",
    "    for name, (model, param_grid) in models.items():\n",
    "        print(f\"\\n{name} Classification Model:\")\n",
    "        best_model = train_and_evaluate_model(X_engineered, y_classification, preprocessor, model, param_grid)\n",
    "        \n",
    "        print(f\"\\n{name} Feature Importance Analysis:\")\n",
    "        if hasattr(best_model.named_steps['model'], 'feature_importances_'):\n",
    "            analyze_feature_importance(best_model, feature_names)\n",
    "        else:\n",
    "            print(\"Feature importance not available for this model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dea2272-12c0-48d7-b5ec-0d9d6fce9565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Modified Notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def preprocess_data(X):\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "def train_and_evaluate_model(X, y, preprocessor, model, param_grid=None):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Apply SMOTE to handle class imbalance\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('model', model)])\n",
    "    \n",
    "    if param_grid:\n",
    "        grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "        best_model = grid_search.best_estimator_\n",
    "    else:\n",
    "        best_model = pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def analyze_feature_importance(model, feature_names):\n",
    "    importances = model.named_steps['model'].feature_importances_\n",
    "    feature_imp = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "    feature_imp = feature_imp.sort_values('importance', ascending=False)\n",
    "    print(feature_imp)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = load_data(r\"C:\\Users\\ADMIN\\Data Analytics Projects\\Student Perfomance Prediction Project\\student_data.csv\")\n",
    "    data['performance_category'] = data['performance_category'].fillna(data['performance_category'].mode()[0])\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = data.drop(['performance', 'performance_category'], axis=1)\n",
    "    y_classification = data['performance_category']\n",
    "    \n",
    "    # Preprocess data\n",
    "    preprocessor = preprocess_data(X)\n",
    "    \n",
    "    # Define models and their parameter grids\n",
    "    models = {\n",
    "        'GradientBoosting': (GradientBoostingClassifier(), {\n",
    "            'model__n_estimators': [100, 200],\n",
    "            'model__learning_rate': [0.01, 0.1],\n",
    "            'model__max_depth': [3, 5]\n",
    "        }),\n",
    "        'SVM': (SVC(), {\n",
    "            'model__C': [0.1, 1, 10],\n",
    "            'model__kernel': ['rbf', 'poly']\n",
    "        }),\n",
    "        'NeuralNetwork': (MLPClassifier(), {\n",
    "            'model__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "            'model__alpha': [0.0001, 0.001, 0.01]\n",
    "        })\n",
    "    }\n",
    "    \n",
    "    for name, (model, param_grid) in models.items():\n",
    "        print(f\"\\n{name} Classification Model:\")\n",
    "        best_model = train_and_evaluate_model(X, y_classification, preprocessor, model, param_grid)\n",
    "        \n",
    "        print(f\"\\n{name} Feature Importance Analysis:\")\n",
    "        if hasattr(best_model.named_steps['model'], 'feature_importances_'):\n",
    "            # Get feature names after preprocessing\n",
    "            feature_names = (preprocessor.named_transformers_['num'].get_feature_names_out().tolist() +\n",
    "                             preprocessor.named_transformers_['cat'].get_feature_names_out().tolist())\n",
    "            analyze_feature_importance(best_model, feature_names)\n",
    "        else:\n",
    "            print(\"Feature importance not available for this model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede58279-128f-49d2-9683-4804c05f5a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def preprocess_data(X):\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "def train_and_evaluate_model(X, y, preprocessor, model, param_grid=None):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create the full pipeline including preprocessing\n",
    "    full_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    if param_grid:\n",
    "        # Update param_grid keys to include 'model__' prefix\n",
    "        updated_param_grid = {'model__' + key: value for key, value in param_grid.items()}\n",
    "        grid_search = GridSearchCV(full_pipeline, updated_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "    else:\n",
    "        best_model = full_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def analyze_feature_importance(model, feature_names):\n",
    "    importances = model.feature_importances_\n",
    "    feature_imp = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "    feature_imp = feature_imp.sort_values('importance', ascending=False)\n",
    "    print(feature_imp)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = load_data(r\"C:\\Users\\ADMIN\\Data Analytics Projects\\Student Perfomance Prediction Project\\student_data.csv\")\n",
    "    data['performance_category'] = data['performance_category'].fillna(data['performance_category'].mode()[0])\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = data.drop(['performance', 'performance_category'], axis=1)\n",
    "    y_classification = data['performance_category']\n",
    "    \n",
    "    # Preprocess data\n",
    "    preprocessor = preprocess_data(X)\n",
    "    \n",
    "    # Define models and their parameter grids\n",
    "    models = {\n",
    "        'GradientBoosting': (GradientBoostingClassifier(), {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'max_depth': [3, 5]\n",
    "        }),\n",
    "        'SVM': (SVC(), {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['rbf', 'poly']\n",
    "        }),\n",
    "        'NeuralNetwork': (MLPClassifier(), {\n",
    "            'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "            'alpha': [0.0001, 0.001, 0.01]\n",
    "        })\n",
    "    }\n",
    "    \n",
    "    for name, (model, param_grid) in models.items():\n",
    "        print(f\"\\n{name} Classification Model:\")\n",
    "        best_model = train_and_evaluate_model(X, y_classification, preprocessor, model, param_grid)\n",
    "        \n",
    "        print(f\"\\n{name} Feature Importance Analysis:\")\n",
    "        if hasattr(best_model.named_steps['model'], 'feature_importances_'):\n",
    "            # Get feature names after preprocessing\n",
    "            preprocessor = best_model.named_steps['preprocessor']\n",
    "            feature_names = (preprocessor.named_transformers_['num'].get_feature_names_out().tolist() +\n",
    "                             preprocessor.named_transformers_['cat'].get_feature_names_out().tolist())\n",
    "            analyze_feature_importance(best_model.named_steps['model'], feature_names)\n",
    "        else:\n",
    "            print(\"Feature importance not available for this model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8df775-eb45-45df-b6c4-abbdb595737c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
